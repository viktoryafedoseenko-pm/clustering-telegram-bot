# analytics.py
"""
–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è Telegram –±–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤
"""
import asyncio
import logging
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
import pandas as pd
from typing import Optional, Tuple
from cache_manager import cache
from pdf_generator import PDFReportGenerator
from config import TEMP_DIR

# Thread pool –¥–ª—è –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
executor = ThreadPoolExecutor(max_workers=2)

logger = logging.getLogger(__name__)

async def generate_detailed_report(
    cache_key: str,
    user_id: int
) -> Optional[Tuple[str, str]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π PDF –æ—Ç—á—ë—Ç –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π CSV
    
    Args:
        cache_key: –ö–ª—é—á –∫—ç—à–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        user_id: Telegram user ID
    
    Returns:
        Tuple[pdf_path, csv_path] –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫—ç—à–∞
    data = cache.load(cache_key)
    if not data:
        logger.error(f"‚ùå Cache not found for key: {cache_key}")
        return None
    
    logger.info(f"‚úÖ Cache loaded: {len(data['df'])} rows, {data['stats']['n_clusters']} clusters")

    df = data['df']
    stats = data['stats']
    cluster_names = data['cluster_names']
    
    # –î–û–ë–ê–í–õ–Ø–ï–ú: –ü–æ–ª—É—á–∞–µ–º –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∫–µ—à–∞
    master_hierarchy = data.get('hierarchy', {})
    master_names = data.get('master_names', {})
    
    logger.info(f"üè∑Ô∏è Master categories: {len(master_hierarchy)} hierarchies, {len(master_names)} names")
    
    # –ü—É—Ç–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pdf_path = TEMP_DIR / f"report_{user_id}_{cache_key[:8]}.pdf"
    csv_path = TEMP_DIR / f"extended_stats_{user_id}_{cache_key[:8]}.csv"
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ (–±–ª–æ–∫–∏—Ä—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏)
    loop = asyncio.get_event_loop()
    
    try:
        # PDF —Å –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        generator = PDFReportGenerator(
            df=df,
            stats=stats, 
            cluster_names=cluster_names,
            master_hierarchy=master_hierarchy,    # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú
            master_names=master_names             # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú
        )
        success = await loop.run_in_executor(
            executor,
            generator.generate,
            str(pdf_path)
        )
        
        if not success:
            return None
        
        # Extended CSV —Å –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        await loop.run_in_executor(
            executor,
            _generate_extended_csv,
            df, cluster_names, str(csv_path), master_hierarchy, master_names  # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú
        )
        
        return str(pdf_path), str(csv_path)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error generating report: {e}")
        return None

def _generate_extended_csv(
    df: pd.DataFrame, 
    cluster_names: dict, 
    output_path: str,
    master_hierarchy: dict = None,
    master_names: dict = None
):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ CSV —Å –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if df.empty:
            logger.error("‚ùå DataFrame is empty")
            raise ValueError("Empty DataFrame")
        
        if 'cluster_id' not in df.columns:
            logger.error(f"‚ùå 'cluster_id' column not found. Available: {df.columns.tolist()}")
            raise ValueError("cluster_id column missing")
        
        # –ü–æ–¥—Å—á—ë—Ç
        logger.info(f"üìä Calculating stats for {len(df)} rows")
        cluster_counts = df['cluster_id'].value_counts().sort_values(ascending=False)
        
        logger.info(f"üìä Found {len(cluster_counts)} clusters")
        logger.info(f"üè∑Ô∏è Master categories: {len(master_hierarchy or {})} hierarchies")
        
        # –°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É
        stats_data = []
        
        for cluster_id, size in cluster_counts.items():
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
            name = cluster_names.get(cluster_id, f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}")
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç
            percent = round((size / len(df)) * 100, 2)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏—é
            master_category = ""
            master_category_id = ""
            master_category_size = 0
            
            if master_hierarchy:
                for master_id, sub_clusters in master_hierarchy.items():
                    if cluster_id in sub_clusters:
                        master_category = master_names.get(master_id, f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {master_id}")
                        master_category_id = master_id
                        # –°—á–∏—Ç–∞–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                        master_category_size = sum(
                            len(df[df['cluster_id'] == cid]) 
                            for cid in sub_clusters
                        )
                        break
            
            stats_data.append({
                'cluster_id': cluster_id,
                'cluster_name': name,
                'master_category_id': master_category_id,
                'master_category_name': master_category,
                'master_category_size': master_category_size,  # –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                'size': int(size),
                'percent': percent
            })
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π
        cluster_stats = pd.DataFrame(stats_data)
        
        if master_hierarchy:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—É–±—ã–≤–∞–Ω–∏–µ), –ø–æ—Ç–æ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∫–ª–∞—Å—Ç–µ—Ä–∞ (—É–±—ã–≤–∞–Ω–∏–µ)
            cluster_stats = cluster_stats.sort_values(
                ['master_category_size', 'size'], 
                ascending=[False, False]
            )
            cluster_stats = cluster_stats.drop('master_category_size', axis=1)
        else:
            cluster_stats = cluster_stats.sort_values('size', ascending=False)
        
        cluster_stats.to_csv(output_path, index=False, encoding='utf-8')
        
        logger.info(f"‚úÖ Extended CSV with sorted master categories saved: {output_path}")
        
    except Exception as e:
        logger.error(f"‚ùå Error in _generate_extended_csv: {e}", exc_info=True)
        raise