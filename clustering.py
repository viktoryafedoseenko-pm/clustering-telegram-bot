# clustering.py
import inspect
import collections

# --- –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Python 3.11+ ---
if not hasattr(inspect, 'getargspec'):
    from collections import namedtuple
    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec
# ------------------------------------

import pandas as pd
import numpy as np
import re
import warnings
import asyncio
from collections import Counter
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer  # +++
import pymorphy2
import os
import requests
import json
from dotenv import load_dotenv
from metrics import ClusteringMetrics


load_dotenv()

morph = pymorphy2.MorphAnalyzer()

#YandexGPT Integration
YANDEX_API_KEY = os.getenv('YANDEX_API_KEY')
YANDEX_FOLDER_ID = os.getenv('YANDEX_FOLDER_ID')

def generate_insight_yandex(stats):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ –∏–Ω—Å–∞–π—Ç–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ YandexGPT
    """
    if not YANDEX_API_KEY or not YANDEX_FOLDER_ID:
        return None

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–Ω—è—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
    n_clusters = stats.get("n_clusters", 0)
    total = stats.get("total_texts", 0)
    noise = stats.get("noise_percent", 0)
    top_clusters = stats.get("top_clusters", [])

    clusters_summary = "\n".join(
        [f"- {c['name']} ‚Äî {c['size']} —Ç–µ–∫—Å—Ç–æ–≤" for c in top_clusters]
    )

    prompt = f"""
–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π.

–í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ {total} —Ç–µ–∫—Å—Ç–æ–≤:
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}
‚Ä¢ –î–æ–ª—è —à—É–º–∞ (–Ω–µ–ø–æ–ø–∞–≤—à–∏—Ö): {noise:.1f}%
‚Ä¢ –¢–æ–ø —Ç–µ–º—ã:
{clusters_summary}

–ó–∞–¥–∞–Ω–∏–µ:
1. –ö—Ä–∞—Ç–∫–æ (–¥–æ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) –æ–±—ä—è—Å–Ω–∏, —á—Ç–æ –≤–∏–¥–Ω–æ –∏–∑ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
2. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-—Ç–æ–Ω, –±–µ–∑ —ç–º–æ–¥–∑–∏.
3. –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ.
"""

    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    headers = {
        "Authorization": f"Api-Key {YANDEX_API_KEY}",
        "Content-Type": "application/json"
    }

    data = {
        "modelUri": f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-lite/latest",
        "completionOptions": {
            "stream": False,
            "temperature": 0.6,
            "maxTokens": 120
        },
        "messages": [{"role": "user", "text": prompt}]
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=10)
        if response.status_code == 200:
            result = response.json()
            text = result['result']['alternatives'][0]['message']['text'].strip()
            return text
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å–∞–π—Ç–∞: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ YandexGPT: {e}")
        return None


def generate_cluster_name_yandex(texts_sample, max_retries=2):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ —á–µ—Ä–µ–∑ YandexGPT
    
    Args:
        texts_sample: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞
        max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
    Returns:
        str: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    if not YANDEX_API_KEY or not YANDEX_FOLDER_ID:
        return None
    
    # –ë–µ—Ä—ë–º 8 –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–æ 130 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥—ã–π)
    examples = "\n".join([f"- {t[:130]}" for t in texts_sample[:8]])
    
    prompt = f"""–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –æ–±—Ä–∞—â–µ–Ω–∏—è –≤ —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫—É.

    –í–æ—Ç –ø—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞—â–µ–Ω–∏–π –∏–∑ –æ–¥–Ω–æ–π —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥—Ä—É–ø–ø—ã:
    {examples}

    –ó–∞–¥–∞–Ω–∏–µ: –ü—Ä–∏–¥—É–º–∞–π —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (2-5 —Å–ª–æ–≤) –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.

    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –ù–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –ë–ï–ó –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Å–ª–æ–≤
    - –ë–µ–∑ —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤
    - –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å–ø–µ—Ü–∏—Ñ–∏–∫—É —ç—Ç–∏—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π
    - –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Ç–µ–º–∞ ‚Äì¬†–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –µ—ë

    ‚úÖ –•–æ—Ä–æ—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã:
    - "–ø—Ä–æ–±–ª–µ–º—ã —Å –æ–ø–ª–∞—Ç–æ–π –∫–∞—Ä—Ç–æ–π" (–Ω–µ –ø—Ä–æ—Å—Ç–æ "–æ–ø–ª–∞—Ç–∞")
    - "–ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–ø–ª–æ–º–∞ –ø–æ –ø–æ—á—Ç–µ" (–Ω–µ –ø—Ä–æ—Å—Ç–æ "–¥–∏–ø–ª–æ–º")
    - "–æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Ö–æ–¥–µ –≤ –∞–∫–∫–∞—É–Ω—Ç" (–Ω–µ –ø—Ä–æ—Å—Ç–æ "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
    - "–≤–æ–ø—Ä–æ—Å—ã –ø–æ –Ω–∞–ª–æ–≥–æ–≤–æ–º—É –≤—ã—á–µ—Ç—É"

    ‚ùå –ü–ª–æ—Ö–∏–µ –ø—Ä–∏–º–µ—Ä—ã (—Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ):
    - "–æ–ø–ª–∞—Ç–∞"
    - "–¥–∏–ø–ª–æ–º"
    - "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"

    –ù–∞–∑–≤–∞–Ω–∏–µ:"""

    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    headers = {
        "Authorization": f"Api-Key {YANDEX_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "modelUri": f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-lite/latest",
        "completionOptions": {
            "stream": False,
            "temperature": 0.4,  
            "maxTokens": 30 
        },
        "messages": [
            {
                "role": "user",
                "text": prompt
            }
        ]
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result['result']['alternatives'][0]['message']['text'].strip()

                text = text.replace('–ù–∞–∑–≤–∞–Ω–∏–µ:', '').strip()
                text = text.strip('"').strip("'")
                
                if len(text) > 50:
                    text = text[:50]
                
                return text
            
            elif response.status_code == 429: 
                print(f"‚ö†Ô∏è Rate limit, –∂–¥—ë–º 2 —Å–µ–∫—É–Ω–¥—ã...")
                import time
                time.sleep(2)
                continue
            
            else:
                print(f"‚ö†Ô∏è YandexGPT –æ—à–∏–±–∫–∞ {response.status_code}: {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"‚ö†Ô∏è YandexGPT timeout (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
            continue
            
        except Exception as e:
            print(f"‚ö†Ô∏è YandexGPT –æ—à–∏–±–∫–∞: {e}")
            return None
    
    return None


# –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
HTML_STOP_WORDS = {
    'style', 'div', 'width', 'height', 'br', 'span', 'class', 'id', 'href', 'src',
    'px', 'pt', 'em', 'rem', 'color', 'background', 'font', 'size', 'border',
    'margin', 'padding', 'align', 'valign', 'center', 'left', 'right', 'justify',
    'table', 'tr', 'td', 'th', 'tbody', 'thead', 'colspan', 'rowspan', 'target',
    'rel', 'nofollow', 'blank', 'www', 'com', 'org', 'net', 'ru', 'quot', 'strong',
    'bold', 'italic', 'underline', 'block', 'inline', 'none', 'hidden', 'visible',
    'display', 'position', 'float', 'clear', 'overflow', 'zindex', 'opacity',
    'img', 'alt', 'title', 'css', 'html', 'body', 'head', 'meta', 'link',
    'ffffff', 'cellspacing', 'cellpadding', 'helvetica', 'arial', 'verdana',
    'usedesk', 'normal', 'variant', 'rgb', 'rgba', 'sans', 'serif', 'blockquote',
    'white', 'space', 'pre', 'wrap', 'text', 'family', 'line', 'height',
    'amp', 'comment_id', 'answer', 'email', 'mailto', 'http', 'https',
    'yandex', 'practicum', 'mail', 'support', 'usedesk', 'ticket', 'weight', 'start transform',
    '255', '000', '111', '222', '333', '444', '555', '666', '777', '888', '999',
}

COMMON_RUSSIAN_STOP_WORDS = {
    '–¥–æ–±—Ä—ã–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–¥–µ–Ω—å', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–ø—Ä–∏–≤–µ—Ç', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
    '—É–≤–∞–∂–∞–µ–º—ã–π', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '—Ö–æ—á—É', '–º–æ–≥—É', '–µ—Å—Ç—å', '–Ω–µ—Ç', '–¥–∞', '–Ω–µ', '–Ω–∞', 
    '–≤', '–∏', '—Å', '—É', '–æ', '–ø–æ', '–∑–∞', '–æ—Ç', '–∏–∑', '–∫', '–¥–æ', '–¥–ª—è', '–∏–ª–∏', '–Ω–æ',
    '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '—Ç–∞–∫', '–≤–æ—Ç', '–∂–µ', '–ª–∏', '–±—ã', '—Ç–æ', '–≤–æ', '—Å–æ', '–∏–∑–æ',
    '–º–µ–Ω—è', '—Ç–µ–±—è', '–µ–≥–æ', '–µ—ë', '–Ω–∞—Å', '–≤–∞—Å', '–∏—Ö', '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à',
    '–≤–∞—à', '–∏—Ö–Ω–∏–π', '–∫—Ç–æ', '—á–µ–≥–æ', '—á–µ–º', '–∫–æ–º—É', '—á–µ–º—É', '–∫–æ–≥–æ', '–µ—â—ë', '—É–∂–µ',
    '–æ—á–µ–Ω—å', '–±–æ–ª–µ–µ', '—Å–∞–º—ã–π', '—Ç–∞–∫–æ–π', '–≤–µ—Å—å', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–∞–∫–æ–π', '—Ç—É—Ç', '—Ç–æ—Ç',
    '–±—É–¥–µ—Ç', '–±—ã–ª–æ', '–±—ã–ª–∏', '–±—É–¥—É', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç–µ', '–±—É–¥—É—Ç',
}

DOMAIN_STOP_WORDS = {
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ
    'usedesk', 'ticket', 'comment', 'answer', 'email', 'support', 'mail', 'mailto',
    'yandex', 'practicum', '–ø—Ä–∞–∫—Ç–∏–∫—É–º', '—è–Ω–¥–µ–∫—Å', '–ø—Ä–∞–∫—Ç–∏–∫—É–º–∞',
    
    # Email
    'sent', 'iphone', 'ipad', 'android', '–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ', 'from', 'gmail',
    '–ø–æ—á—Ç–∞', '–ø–æ—á—Ç—É', '–ø–∏—Å—å–º–æ', '–ø–∏—Å—å–º–∞',
    
    # –î–∞—Ç—ã
    '—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è',
    '–∏—é–ª—è', '–∞–≤–≥—É—Å—Ç–∞', '—Å–µ–Ω—Ç—è–±—Ä—è', '–æ–∫—Ç—è–±—Ä—è', '–Ω–æ—è–±—Ä—è', '–¥–µ–∫–∞–±—Ä—è',
    '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–≤—Ç–æ—Ä–Ω–∏–∫', '—Å—Ä–µ–¥–∞', '—á–µ—Ç–≤–µ—Ä–≥', '–ø—è—Ç–Ω–∏—Ü–∞', '—Å—É–±–±–æ—Ç–∞', '–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ',
    '—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–∑–∞–≤—Ç—Ä–∞',
    
    # HTML/CSS (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫!)
    'content', 'noreferrer', 'noopener', 'secure', 'nps', 'important', 'nbsp',
    'bgcolor', 'radius', 'display', 'block', 'inline', 'hidden', 'visible',
    'opacity', 'overflow', 'target', 'blank', 'rel', 'href', 'src', 'alt',
    'title', 'class', 'style', 'font', 'margin', 'padding', 'border',
    'width', 'height', 'px', 'caps', 'start', 'word', 'decoration', 'break', 'transparent', 'inbound', 'blank',
    'transform', 'lesson', 'max', 'min', 'px',
    
    # UTM –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    'utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 'utm_term',
    
    # –û–±—â–∏–µ —Ñ—Ä–∞–∑—ã
    '–º–Ω–µ', '–º–µ–Ω—è', '–≤–∞–º', '–≤–∞—Å', '–Ω–∞—Å', '—Ç–µ–±—è', '–µ–≥–æ', '–µ—ë',
    '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '—Ö–æ—á—É', '–º–æ–≥—É', '—Ö–æ—Ç–µ–ª', '—Ö–æ—Ç–µ–ª–∞', '–Ω–∞–¥–æ',
    '—Å–µ–π—á–∞—Å', '—Ç–µ–ø–µ—Ä—å', '—É–∂–µ', '–µ—â—ë', '–≤–æ–ø—Ä–æ—Å', '–ø–æ–º–æ—á—å', '–ø–æ–º–æ–≥–∏—Ç–µ',
    '–¥–æ–±—Ä–æ–µ', '—É—Ç—Ä–æ', '–≤–µ—á–µ—Ä', '–Ω–æ—á—å',  # "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ ‚Ä¢ —É—Ç—Ä–æ ‚Ä¢ –¥–æ–±—Ä–æ–µ"

    # –ß–∏—Å–ª–∞ –∏ –∫–æ–¥—ã
    '2025', '2024', '00', '07', '06', '01', '02', '03', '04', '05', '08', '09', '10', '11', '12',
    '540px', '15', '20', '30',

    # –°–ª—É–∂–µ–±–Ω—ã–µ
    'message', '—Å—É–º–º—É', '—á–µ–∫',  # "message ‚Ä¢ 00 —Å—É–º–º—É"
}

STOP_WORDS = COMMON_RUSSIAN_STOP_WORDS.union(HTML_STOP_WORDS).union(DOMAIN_STOP_WORDS)

MINIMAL_STOP_WORDS = {
    # –°–ª—É–∂–µ–±–Ω—ã–µ
    '–Ω–∞', '–≤', '–∏', '—Å', '—É', '–æ', '–ø–æ', '–∑–∞', '–æ—Ç', '–∏–∑', '–∫', '–¥–æ',
    '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '—Ç–∞–∫', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ',
    
    # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å
    '–¥–æ–±—Ä—ã–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
    
    # HTML/—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ
    'usedesk', 'ticket', 'email', 'nbsp', 'amp', 'quot',
    'width', 'height', 'style', 'div', 'span', 'br',
    
    # –Ø–Ω–¥–µ–∫—Å –ü—Ä–∞–∫—Ç–∏–∫—É–º (–µ—Å–ª–∏ –Ω–µ –≤–∞–∂–Ω—ã)
    'yandex', 'practicum', '—è–Ω–¥–µ–∫—Å', '–ø—Ä–∞–∫—Ç–∏–∫—É–º',
}

morph = pymorphy2.MorphAnalyzer()

def clean_html(text: str) -> str:
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ HTML –∏ CSS"""
    if not isinstance(text, str):
        return ""
    
    # 1. –£–¥–∞–ª—è–µ–º email-–ø–æ–¥–ø–∏—Å–∏ –∏ —à–∞–±–ª–æ–Ω—ã
    email_patterns = [
        r'–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å (iPhone|iPad|Android|–º–æ–±–∏–ª—å–Ω\w+)',
        r'Sent from (my )?iPhone',
        r'--\s*–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∏–∑.*?–ü–æ—á—Ç—ã',
        r'–°–ª—É–∂–±–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏.*',
        r'T_I_C_K_E_T_I_D_\d+',
        r'U_D_I_D_\d+',
        r'–û—Ü–µ–Ω–∏—Ç–µ.*–Ω–∞—à—É –ø–æ–¥–¥–µ—Ä–∂–∫—É:.*',
        r'–≠—Ç–æ –ø–∏—Å—å–º–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ–ø—Ä–æ—Å.*',
        r'–Ø–Ω–¥–µ–∫—Å –Ω–µ –Ω–µ—Å—ë—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏.*',
    ]
    for pattern in email_patterns:
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE | re.DOTALL)
    
    # 2. HTML-—Ç–µ–≥–∏ –∏ entities
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'&[a-z]+;', ' ', text)
    text = re.sub(r'&#\d+;', ' ', text)
    
    # 3. CSS-—Å—Ç–∏–ª–∏ (—É—Å–∏–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    text = re.sub(r'[a-z\-]+\s*:\s*[^;"]+;?', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'\w+\s*=\s*["\'][^"\']*["\']', ' ', text)
    text = re.sub(r'\b\d+[a-z%]+\b', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'#[0-9a-f]{3,6}\b', ' ', text, flags=re.IGNORECASE)
    
    # 4. –£–¥–∞–ª—è–µ–º HTML/CSS —Å–ª–æ–≤–∞-–º—É—Å–æ—Ä (–ù–û–í–´–ô –ë–õ–û–ö!)
    html_junk = [
        r'\bcontent\b', r'\bnoreferrer\b', r'\bnoopener\b', r'\bsecure\b',
        r'\bnps\b', r'\bimportant\b', r'\bnbsp\b', r'\bbgcolor\b',
        r'\bradius\b', r'\bdisplay\b', r'\bblock\b', r'\binline\b',
        r'\bhidden\b', r'\bvisible\b', r'\bopacity\b', r'\boverflow\b',
        r'\btarget\b', r'\bblank\b', r'\brel\b', r'\bhref\b', r'\bsrc\b',
        r'\balt\b', r'\btitle\b', r'\bclass\b', r'\bid\b',
    ]
    for pattern in html_junk:
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)
    
    # 5. –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —á–∏—Å–ª–∞ –∏ CSS-—Å–ª–æ–≤–∞
    text = re.sub(r'\b(\d+)\s+\1\b', '', text) 
    text = re.sub(r'\b\d+px\b', '', text, flags=re.I)
    text = re.sub(r'\bcaps\b', '', text, flags=re.I)
    text = re.sub(r'\bstart\b', '', text, flags=re.I)
    
    # 6. –£–¥–∞–ª—è–µ–º –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è (–∏–∑ —Ñ–æ—Ä–º)
    text = re.sub(r'_{3,}', ' ', text)  # _______
    text = re.sub(r'_+', ' ', text)     # –õ—é–±—ã–µ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
    
    # 7. –£–¥–∞–ª—è–µ–º "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:"
    text = re.sub(r'–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:.*', '', text, flags=re.I)

    # 8. –ß–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def preprocess_text(text: str) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    if not isinstance(text, str) or not text.strip():
        return ""
    
    text = clean_html(text)
    text = text.lower()
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    words = []
    for w in text.split():
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        if (len(w) > 2 and 
            len(w) < 20 and 
            w not in STOP_WORDS and
            not w.isdigit() and
            not re.match(r'^\d+$', w) and
            not re.match(r'^\d+[a-z]+$', w, re.I) and 
            not re.match(r'^[a-z]+\d+$', w, re.I) and
            not any(bad in w for bad in ['amp', 'comment', 'answer', 'mailto'])):
            
            # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤
            if re.match(r'^[–∞-—è—ë]+$', w):
                parsed = morph.parse(w)[0]
                w = parsed.normal_form
            words.append(w)
    
    return ' '.join(words)

def merge_similar_clusters(topics, topic_model, df, similarity_threshold=0.75):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    
    Args:
        topics: –º–∞—Å—Å–∏–≤ cluster_id –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        topic_model: –æ–±—É—á–µ–Ω–Ω–∞—è BERTopic –º–æ–¥–µ–ª—å
        df: –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —Ç–µ–∫—Å—Ç–∞–º–∏
        similarity_threshold: –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1), –≤—ã—à–µ = —Å—Ç—Ä–æ–∂–µ
    
    Returns:
        topics: –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ cluster_id
        merge_map: dict {old_id: new_id}
    """
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    
    # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–±–µ–∑ —à—É–º–∞)
    unique_clusters = [c for c in set(topics) if c != -1]
    
    if len(unique_clusters) < 2:
        return topics, {}
    
    # –î–æ—Å—Ç–∞—ë–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    # BERTopic –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ Backend, –Ω—É–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
    embedding_model = topic_model.embedding_model
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞—Ç—Ä–∏–±—É—Ç .embedding_model (–µ—Å–ª–∏ —ç—Ç–æ Backend)
    if hasattr(embedding_model, 'embedding_model'):
        embedding_model = embedding_model.embedding_model
    
    # –ü–æ–ª—É—á–∞–µ–º embeddings —Ü–µ–Ω—Ç—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    cluster_centers = {}
    
    for cluster_id in unique_clusters:
        cluster_indices = [i for i, c in enumerate(topics) if c == cluster_id]
        if len(cluster_indices) == 0:
            continue
        
        # –ë–µ—Ä—ë–º —Ç–µ–∫—Å—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∞
        cluster_texts = [df.iloc[i, 0] for i in cluster_indices[:20]]  # –º–∞–∫—Å 20 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        # –ü–æ–ª—É—á–∞–µ–º embeddings
        embeddings = embedding_model.encode(cluster_texts)
        
        # –¶–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ = —Å—Ä–µ–¥–Ω–µ–µ embeddings
        cluster_centers[cluster_id] = np.mean(embeddings, axis=0)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    cluster_ids = list(cluster_centers.keys())
    center_vectors = np.array([cluster_centers[cid] for cid in cluster_ids])
    
    similarity_matrix = cosine_similarity(center_vectors)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    merge_map = {}  # {old_id: new_id}
    
    for i in range(len(cluster_ids)):
        for j in range(i + 1, len(cluster_ids)):
            similarity = similarity_matrix[i][j]
            
            if similarity >= similarity_threshold:
                cluster_i = cluster_ids[i]
                cluster_j = cluster_ids[j]
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä —Å –º–µ–Ω—å—à–∏–º ID
                target = min(cluster_i, cluster_j)
                source = max(cluster_i, cluster_j)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ source —É–∂–µ –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–µ–Ω
                if source in merge_map:
                    continue
                
                merge_map[source] = target
                print(f"üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã {source} ‚Üí {target} (—Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.2f})")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    if merge_map:
        topics_merged = topics.copy()
        for i, cluster_id in enumerate(topics):
            if cluster_id in merge_map:
                topics_merged[i] = merge_map[cluster_id]
        
        print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(merge_map)} –ø–∞—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return topics_merged, merge_map
    
    return topics, {}


def calculate_metrics(topics, cluster_names, topic_model):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    cluster_counts = Counter(topics)
    noise_count = cluster_counts.get(-1, 0)
    noise_percent = (noise_count / len(topics)) * 100 if len(topics) > 0 else 0
    n_clusters = len([c for c in cluster_counts.keys() if c != -1])
    cluster_sizes = [count for cluster, count in cluster_counts.items() if cluster != -1]
    avg_size = np.mean(cluster_sizes) if cluster_sizes else 0
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ø –∫–ª–∞—Å—Ç–µ—Ä—ã —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    top_clusters = []
    seen_names = set()
    
    sorted_clusters = sorted(
        [(cluster, count) for cluster, count in cluster_counts.items() if cluster != -1],
        key=lambda x: x[1],
        reverse=True
    )
    
    for cluster_id, size in sorted_clusters:
        name = cluster_names.get(cluster_id, f"Cluster {cluster_id}")
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –Ω–∞–∑–≤–∞–Ω–∏—è
        if name in seen_names:
            continue
            
        seen_names.add(name)
        top_clusters.append({
            'id': cluster_id,
            'name': name,
            'size': size
        })
        
        if len(top_clusters) >= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–æ–ø-3
            break
    
    return {
        'n_clusters': n_clusters,
        'noise_percent': round(noise_percent, 2),
        'avg_cluster_size': round(avg_size, 2),
        'total_texts': len(topics),
        'top_clusters': top_clusters,
        'cluster_distribution': dict(cluster_counts)
    }


def clusterize_texts(file_path: str, progress_callback=None):
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    import time
    start_time = time.time()

    async def log_progress(msg):
        print(msg)
        if progress_callback:
            try:
                await progress_callback(msg)
            except:
                pass

    def sync_log(msg):
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(log_progress(msg))
            else:
                loop.run_until_complete(log_progress(msg))
        except:
            print(msg)

    # –ó–∞–≥—Ä—É–∑–∫–∞
    sync_log("üì• –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª...")
    df = pd.read_csv(file_path, usecols=[0], encoding='utf-8', dtype=str)
    raw_texts = df.iloc[:, 0].fillna("").astype(str).tolist()
    n = len(raw_texts)
    if n == 0:
        raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π")

    sync_log(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {n} —Ç–µ–∫—Å—Ç–æ–≤")

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    sync_log("üßπ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞...")
    preprocessed_texts = [preprocess_text(t) for t in raw_texts]

    valid_indices = [i for i, t in enumerate(preprocessed_texts) 
                    if t.strip() and len(t.split()) >= 2]

    if len(valid_indices) <= 3:
        df["cluster_id"] = 0
        df["cluster_name"] = "–í—Å–µ —Ç–µ–∫—Å—Ç—ã"
        out = file_path.replace(".csv", "_clustered.csv")
        df.to_csv(out, index=False, encoding='utf-8')
        return out, {'n_clusters': 1, 'total_texts': n}

    preprocessed_texts = [preprocessed_texts[i] for i in valid_indices]
    df = df.iloc[valid_indices].reset_index(drop=True)

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –æ—á–∏—â–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º
    sync_log("üîç –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    df['_preprocessed'] = preprocessed_texts
    df = df.drop_duplicates(subset='_preprocessed', keep="first").reset_index(drop=True)
    preprocessed_texts = df['_preprocessed'].tolist()
    df = df.drop(columns=['_preprocessed'])
    unique_texts = preprocessed_texts
    n_unique = len(unique_texts)
    sync_log(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {n_unique}")

    # üÜï –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–ü–û–°–õ–ï —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤!)
    print(f"\nüîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–ï–†–ï–î VECTORIZER:")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {n_unique}")
    print(f"   –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: '{unique_texts[0][:100]}'" if unique_texts else "–ü–£–°–¢–û")

    # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞
    avg_length = sum(len(t.split()) for t in unique_texts) / len(unique_texts) if unique_texts else 0
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {avg_length:.1f} —Å–ª–æ–≤")

    # –°—á–∏—Ç–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –í–°–ï–• unique_texts
    all_words = []
    for text in unique_texts:
        all_words.extend(text.split())

    unique_words = set(all_words)
    total_words = len(all_words)
    print(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(unique_words)}")
    print(f"   –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤: {list(unique_words)[:30]}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
    empty_count = sum(1 for t in unique_texts if len(t.split()) == 0)
    print(f"   –ü—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {empty_count}")

    # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    sync_log("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞...")
    def create_vectorizer(unique_words, n_unique):
        """–°–æ–∑–¥–∞—ë—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç CountVectorizer —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        
        def safe_vectorizer_params(n_docs: int, n_words: int):
            """
            –ü–æ–¥–±–∏—Ä–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ min_df –∏ max_df –¥–ª—è CountVectorizer.
            –†–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (<10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤).
            """
            # –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if n_docs < 50:
                min_df, max_df = 1, 1.0
            elif n_docs < 200:
                min_df, max_df = 2, 0.9
            elif n_docs < 1000:
                min_df, max_df = 3, 0.8
            else:
                min_df, max_df = 5, 0.7

            # –û—Å–ª–∞–±–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã, –µ—Å–ª–∏ —Å–ª–æ–≤ –º–∞–ª–æ
            if n_words < 30:
                min_df, max_df = 1, 1.0
            elif n_words < 100:
                min_df, max_df = 1, 0.95

            # ü©π –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
            if isinstance(max_df, float):
                if max_df * n_docs < min_df:
                    max_df = min(1.0, min_df / n_docs + 0.05)
            elif isinstance(max_df, int) and max_df < min_df:
                max_df = min_df + 1

            return min_df, max_df

        # === –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ===
        n_unique_words = len(unique_words)
        min_df, max_df = safe_vectorizer_params(n_unique, n_unique_words)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –∫–æ—Ä–ø—É—Å–∞
        if n_unique_words < 30:
            config = {
                'ngram_range': (1, 2),
                'stop_words': None,
                'max_features': 500
            }
        elif n_unique_words < 100:
            config = {
                'ngram_range': (1, 2),
                'stop_words': None,
                'max_features': 1000
            }
        else:
            config = {
                'ngram_range': (1, 2),
                'stop_words': list(MINIMAL_STOP_WORDS),
                'max_features': 1000
            }

        # –°–æ–∑–¥–∞—ë–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π CountVectorizer
        vectorizer = CountVectorizer(
            **config,
            min_df=min_df,
            max_df=max_df
        )

        print(f"   ‚úÖ Vectorizer —Å–æ–∑–¥–∞–Ω: min_df={min_df}, max_df={max_df}")
        return vectorizer

    vectorizer_model = create_vectorizer(unique_words, n_unique)
    print(f"   ‚úÖ Vectorizer —Å–æ–∑–¥–∞–Ω\n")

    # –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    sync_log("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    if n_unique < 500:
        # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        min_cluster_size = 5
        min_samples = 2
        n_neighbors = 10
    elif n_unique < 5000:
        min_cluster_size = max(12, int(n_unique * 0.015))
        min_samples = 2 
        n_neighbors = min(35, max(25, n_unique // 25)) 
    else:
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (30–∫+)
        min_cluster_size = max(50, int(n_unique * 0.002))  # ~60 –¥–ª—è 30–∫
        min_samples = max(10, int(min_cluster_size * 0.2)) # ~12
        n_neighbors = min(50, max(30, n_unique // 200))    # ~150

    # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {n_unique} —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"   min_cluster_size = {min_cluster_size}")
    print(f"   min_samples = {min_samples}")
    print(f"   n_neighbors = {n_neighbors}")

    n_components = 10


    umap_model = UMAP(
        n_neighbors=n_neighbors,
        n_components=n_components,
        min_dist=0.0,
        metric='cosine',
        random_state=42,
        n_jobs=1
    )
    
    hdbscan_model = HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True,
        cluster_selection_epsilon=0.3
    )
    
    topic_model = BERTopic(
        embedding_model=model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer_model,
        language="multilingual",
        calculate_probabilities=False,
        verbose=False,
        top_n_words=10,
        n_gram_range=(1, 2),
        min_topic_size=min_cluster_size
    )

    # –§–∏–ª—å—Ç—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤
    def filter_topic_words(topic_words, banned_words):
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
        filtered = []
        for word, score in topic_words:
            w_lower = word.lower()
            # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            if (w_lower not in banned_words and
                len(word) > 2 and
                len(word) < 20 and
                not word.isdigit() and
                not re.match(r'^\d+[a-z%]*$', word, re.I) and
                not re.match(r'^[a-z]{1,3}$', word)):  # –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–Ω–≥–ª —Å–ª–æ–≤–∞
                filtered.append((word, score))
            if len(filtered) >= 5:  # –ë–µ—Ä—ë–º —Ç–æ–ø-5 —Å–ª–æ–≤
                break
        return filtered


    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    sync_log(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (min_size={min_cluster_size})...")
    try:
        topics, _ = topic_model.fit_transform(unique_texts)
    except Exception as e:
        sync_log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        raise

    # üÜï –ü–û–õ–£–ß–ê–ï–ú EMBEDDINGS –î–õ–Ø –ú–ï–¢–†–ò–ö
    sync_log("üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞...")
    embeddings = topic_model._extract_embeddings(
        unique_texts,
        method="document"
    )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∏–ª–∏)
    # sync_log("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    # topics, merge_map = merge_similar_clusters(
    #     topics, 
    #     topic_model, 
    #     pd.DataFrame({0: unique_texts}),
    #     similarity_threshold=0.70
    # )

    # –û–±–Ω–æ–≤–ª—è–µ–º topic_model.get_topic_info() –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    # BERTopic –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ç–æ–ø–∏–∫–∏
    # if merge_map:
    #      sync_log("üìä –ü–µ—Ä–µ—Å—á—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è...")
    #     # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ø–∏–∫–∏ –≤ –º–æ–¥–µ–ª–∏
    #    topic_model.topics_ = topics
    #    sync_log(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(merge_map)} –ø–∞—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

    quality_metrics = ClusteringMetrics.calculate(embeddings, topics)
    sync_log(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏: Silhouette={quality_metrics['silhouette_score']:.3f}, DB={quality_metrics['davies_bouldin_index']:.3f}")

    # –ù–∞–∑–≤–∞–Ω–∏—è (—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π)
    if YANDEX_API_KEY and YANDEX_FOLDER_ID:
        sync_log("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å –ø–æ–º–æ—â—å—é YandexGPT...")
    else:
        sync_log("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π...")

    info = topic_model.get_topic_info()
    cluster_names = {}

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    unique_clusters = set(topics)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ topics!

    for cluster_id in unique_clusters:
        if cluster_id == -1:
            cluster_names[cluster_id] = "–ü—Ä–æ—á–µ–µ"
            continue

    # –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    unique_clusters = set(topics)
    for cluster_id in unique_clusters:
        if cluster_id == -1:
            cluster_names[cluster_id] = "üîπ –ü—Ä–æ—á–µ–µ"
            continue
        
        topic_words = topic_model.get_topic(cluster_id)
        if not topic_words:
            cluster_names[cluster_id] = f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}"
            continue
        
        # 1. –ü—Ä–æ–±—É–µ–º YandexGPT (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)
        if YANDEX_API_KEY and YANDEX_FOLDER_ID:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_texts = [unique_texts[i] for i, cluster_id_enum in enumerate(topics) if cluster_id_enum == cluster_id]
            
            if cluster_texts:
                yandex_name = generate_cluster_name_yandex(cluster_texts)
                if yandex_name:
                    print(f"‚ú® –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {yandex_name}")
                    cluster_names[cluster_id] = yandex_name
                    continue
        
        # 2. Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º BERTopic —Å–ª–æ–≤–∞
        filtered = filter_topic_words(topic_words, ALL_STOP_WORDS)
        
        if filtered:
            name = " ‚Ä¢ ".join([w for w, s in filtered[:3]])
            cluster_names[cluster_id] = name
        else:
            cluster_names[cluster_id] = f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}"

    # –¢–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç–æ –º–∞–ø–ø–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∫ –Ω–∞–∑–≤–∞–Ω–∏—è–º
    df["cluster_id"] = topics
    df["cluster_name"] = [cluster_names[t] for t in topics]

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    import re

    # clustering.py
    def normalize_cluster_name(name: str) -> str:
        """–õ—ë–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî —Ç–æ–ª—å–∫–æ –æ—á–µ–≤–∏–¥–Ω—ã–µ –¥—É–±–ª–∏"""
        if not isinstance(name, str):
            return ""
        
        name = name.lower().strip()
        name = re.sub(r'[¬´¬ª"\'üîπ‚Ä¢]', '', name)
        name = re.sub(r'[^–∞-—è—ëa-z0-9\s-]', ' ', name)  # –†–∞–∑—Ä–µ—à–∞–µ–º –¥–µ—Ñ–∏—Å
        name = re.sub(r'\s+', ' ', name).strip()

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞–º–µ–Ω—ã ‚Äî —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –¥—É–±–ª–∏
        replacements = {
            '–¥–∏–ø–ª–æ–º—ã': '–¥–∏–ø–ª–æ–º',
            '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã': '–¥–∏–ø–ª–æ–º',
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏',
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–±–æ–∏': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏',
        }
        
        for old, new in replacements.items():
            if name == old:  # ‚Üê –¢–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ!
                name = new
        
        return name.title()

    df["cluster_name"] = df["cluster_name"].apply(normalize_cluster_name)
    df["cluster_name"] = df["cluster_name"].apply(lambda x: x.capitalize() if isinstance(x, str) else x)

    # –ú–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ llm
    def generate_master_categories_yandex(cluster_names):
        # —Å–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        examples = "\n".join([f"- {n}" for n in cluster_names])

        prompt = f"""
    –¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –æ–±—Ä–∞—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã.
    –ü–µ—Ä–µ–¥ —Ç–æ–±–æ–π —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—Ç–µ–º) –æ–±—Ä–∞—â–µ–Ω–∏–π.
    –ù—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏–π (4‚Äì8 —à—Ç—É–∫).

    –ü—Ä–∞–≤–∏–ª–∞:
    - –ì—Ä—É–ø–ø–∏—Ä—É–π —Ç–æ–ª—å–∫–æ –ø–æ —Å–º—ã—Å–ª—É.
    - –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º (2‚Äì4 —Å–ª–æ–≤–∞).
    - –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ, –∫—Ä–æ–º–µ JSON.
    - –í–µ—Ä–Ω–∏ JSON, –≥–¥–µ –∫–ª—é—á ‚Äî –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∞ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤ –Ω–µ—ë –≤—Ö–æ–¥—è—Ç.

    –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞:
    {{
    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã": ["–û–ø–ª–∞—Ç–∞", "–ü—Ä–æ–±–ª–µ–º—ã —Å –æ–ø–ª–∞—Ç–æ–π"],
    "–î–æ–∫—É–º–µ–Ω—Ç—ã –∏ –¥–∏–ø–ª–æ–º—ã": ["–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–ø–ª–æ–º–∞", "–î–∏–ø–ª–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã"],
    "–£—á–µ–±–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã": ["–ü–æ–º–æ—â—å —Å –∫—É—Ä—Å–∞–º–∏", "–í–æ–ø—Ä–æ—Å—ã –ø–æ –æ–±—É—á–µ–Ω–∏—é"]
    }}

    –í–æ—Ç —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏:
    {examples}
    """

        url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
        headers = {
            "Authorization": f"Api-Key {YANDEX_API_KEY}",
            "Content-Type": "application/json"
        }
        data = {
            "modelUri": f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-lite/latest",
            "completionOptions": {"stream": False, "temperature": 0.3, "maxTokens": 700},
            "messages": [{"role": "user", "text": prompt}]
        }

        response = requests.post(url, headers=headers, json=data)
        text = response.json()["result"]["alternatives"][0]["message"]["text"]

        # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
        try:
            parsed = json.loads(text)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, –æ—Ç–≤–µ—Ç LLM:")
            print(text)
            parsed = {"–ü—Ä–æ—á–µ–µ": cluster_names}
        return parsed


    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    unique_names = df["cluster_name"].dropna().unique().tolist()
    auto_categories = generate_master_categories_yandex(unique_names)

    # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Ç–µ—Ä–∞ -> –º–∞—Å—Ç–µ—Ä-–∫–∞—Ç–µ–≥–æ—Ä–∏—è
    def map_to_master(name):
        for cat, subs in auto_categories.items():
            if name in subs:
                return cat
        return "–ü—Ä–æ—á–µ–µ"

    df["master_category"] = df["cluster_name"].apply(map_to_master)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    stats = (
        df.groupby("master_category")
        .agg(–ö–ª–∞—Å—Ç–µ—Ä–æ–≤=("cluster_name", "nunique"))
        .sort_values("–ö–ª–∞—Å—Ç–µ—Ä–æ–≤", ascending=False)
    )
    print(stats)

    # –ú–µ—Ç—Ä–∏–∫–∏
    stats = calculate_metrics(topics, cluster_names, topic_model)
    sync_log(f"‚úÖ {stats['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∑–∞ {time.time()-start_time:.1f}—Å")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    out = file_path.replace(".csv", "_clustered.csv")
    df.to_csv(out, index=False, encoding='utf-8')

    stats = calculate_metrics(topics, cluster_names, topic_model)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ stats
    stats['quality_metrics'] = quality_metrics
    
    sync_log(f"‚úÖ {stats['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∑–∞ {time.time()-start_time:.1f}—Å")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    out = file_path.replace(".csv", "_clustered.csv")
    df.to_csv(out, index=False, encoding='utf-8')
    
    return out, stats