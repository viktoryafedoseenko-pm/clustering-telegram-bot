# clustering.py
import inspect
import collections

# --- –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Python 3.11+ ---
if not hasattr(inspect, 'getargspec'):
    from collections import namedtuple
    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec
# ------------------------------------

import pandas as pd
import numpy as np
import re
import warnings
import asyncio
from collections import Counter
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer  # +++
import pymorphy2

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
HTML_STOP_WORDS = {
    # HTML/CSS –±–∞–∑–æ–≤—ã–µ
    'style', 'div', 'width', 'height', 'br', 'span', 'class', 'id', 'href', 'src',
    'px', 'pt', 'em', 'rem', 'color', 'background', 'font', 'size', 'border',
    'margin', 'padding', 'align', 'valign', 'center', 'left', 'right', 'justify',
    'table', 'tr', 'td', 'th', 'tbody', 'thead', 'colspan', 'rowspan', 'target',
    'rel', 'nofollow', 'blank', 'www', 'com', 'org', 'net', 'ru', 'quot', 'strong',
    'bold', 'italic', 'underline', 'block', 'inline', 'none', 'hidden', 'visible',
    'display', 'position', 'float', 'clear', 'overflow', 'zindex', 'opacity',
    'img', 'alt', 'title', 'css', 'html', 'body', 'head', 'meta', 'link',
    'ffffff', 'cellspacing', 'cellpadding', 'helvetica', 'arial', 'verdana',
    'usedesk', 'normal', 'variant', 'rgb', 'rgba', 'sans', 'serif', 'blockquote',
    'white', 'space', 'pre', 'wrap', 'text', 'family', 'line', 'height',
    'amp', 'comment_id', 'answer', 'email', 'mailto', 'http', 'https',
    'yandex', 'practicum', 'mail', 'support', 'usedesk', 'ticket', 'weight', 'start transform',
    '255', '000', '111', '222', '333', '444', '555', '666', '777', '888', '999',
}

COMMON_RUSSIAN_STOP_WORDS = {
    '–¥–æ–±—Ä—ã–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–¥–µ–Ω—å', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–ø—Ä–∏–≤–µ—Ç', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
    '—É–≤–∞–∂–∞–µ–º—ã–π', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '—Ö–æ—á—É', '–º–æ–≥—É', '–µ—Å—Ç—å', '–Ω–µ—Ç', '–¥–∞', '–Ω–µ', '–Ω–∞', 
    '–≤', '–∏', '—Å', '—É', '–æ', '–ø–æ', '–∑–∞', '–æ—Ç', '–∏–∑', '–∫', '–¥–æ', '–¥–ª—è', '–∏–ª–∏', '–Ω–æ',
    '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '—Ç–∞–∫', '–≤–æ—Ç', '–∂–µ', '–ª–∏', '–±—ã', '—Ç–æ', '–≤–æ', '—Å–æ', '–∏–∑–æ',
    '–º–µ–Ω—è', '—Ç–µ–±—è', '–µ–≥–æ', '–µ—ë', '–Ω–∞—Å', '–≤–∞—Å', '–∏—Ö', '–º–æ–π', '—Ç–≤–æ–π', '—Å–≤–æ–π', '–Ω–∞—à',
    '–≤–∞—à', '–∏—Ö–Ω–∏–π', '–∫—Ç–æ', '—á–µ–≥–æ', '—á–µ–º', '–∫–æ–º—É', '—á–µ–º—É', '–∫–æ–≥–æ', '–µ—â—ë', '—É–∂–µ',
    '–æ—á–µ–Ω—å', '–±–æ–ª–µ–µ', '—Å–∞–º—ã–π', '—Ç–∞–∫–æ–π', '–≤–µ—Å—å', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–∞–∫–æ–π', '—Ç—É—Ç', '—Ç–æ—Ç',
    '–±—É–¥–µ—Ç', '–±—ã–ª–æ', '–±—ã–ª–∏', '–±—É–¥—É', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç–µ', '–±—É–¥—É—Ç',
}

# –ù–æ–≤—ã–π –±–ª–æ–∫ ‚Äî –¥–æ–±–∞–≤—å –ø–æ—Å–ª–µ COMMON_RUSSIAN_STOP_WORDS
DOMAIN_STOP_WORDS = {
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ
    'usedesk', 'ticket', 'comment', 'answer', 'email', 'support', 'mail', 'mailto',
    'yandex', 'practicum', '–ø—Ä–∞–∫—Ç–∏–∫—É–º', '—è–Ω–¥–µ–∫—Å',
    
    # Email
    'sent', 'iphone', 'ipad', 'android', '–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ', 'from', 'gmail',
    '–ø–æ—á—Ç–∞', '–ø–æ—á—Ç—É', '–ø–∏—Å—å–º–æ', '–ø–∏—Å—å–º–∞',
    
    # –î–∞—Ç—ã (—É–±–∏—Ä–∞–µ–º –∏—é–ª—è, —Å–ø—Ä–∏–Ω—Ç –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤!)
    '—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è',
    '–∏—é–ª—è', '–∞–≤–≥—É—Å—Ç–∞', '—Å–µ–Ω—Ç—è–±—Ä—è', '–æ–∫—Ç—è–±—Ä—è', '–Ω–æ—è–±—Ä—è', '–¥–µ–∫–∞–±—Ä—è',
    '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–≤—Ç–æ—Ä–Ω–∏–∫', '—Å—Ä–µ–¥–∞', '—á–µ—Ç–≤–µ—Ä–≥', '–ø—è—Ç–Ω–∏—Ü–∞', '—Å—É–±–±–æ—Ç–∞', '–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ',
    '—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–∑–∞–≤—Ç—Ä–∞', '—Å–ø—Ä–∏–Ω—Ç',
}

STOP_WORDS = COMMON_RUSSIAN_STOP_WORDS.union(HTML_STOP_WORDS).union(DOMAIN_STOP_WORDS)

morph = pymorphy2.MorphAnalyzer()

def clean_html(text: str) -> str:
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ HTML –∏ CSS"""
    if not isinstance(text, str):
        return ""
    
    # 1. –£–¥–∞–ª—è–µ–º email-–ø–æ–¥–ø–∏—Å–∏ –∏ —à–∞–±–ª–æ–Ω—ã
    email_patterns = [
        r'–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å (iPhone|iPad|Android|–º–æ–±–∏–ª—å–Ω\w+)',
        r'Sent from (my )?iPhone',
        r'--\s*–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∏–∑.*?–ü–æ—á—Ç—ã',
        r'–°–ª—É–∂–±–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏.*',
        r'T_I_C_K_E_T_I_D_\d+',
        r'U_D_I_D_\d+',
        r'–û—Ü–µ–Ω–∏—Ç–µ.*–Ω–∞—à—É –ø–æ–¥–¥–µ—Ä–∂–∫—É:.*',
        r'–≠—Ç–æ –ø–∏—Å—å–º–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ–ø—Ä–æ—Å.*',
        r'–Ø–Ω–¥–µ–∫—Å –Ω–µ –Ω–µ—Å—ë—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏.*',
    ]
    for pattern in email_patterns:
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE | re.DOTALL)
    
    # 2. HTML-—Ç–µ–≥–∏ –∏ entities
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'&[a-z]+;', ' ', text)
    text = re.sub(r'&#\d+;', ' ', text)
    
    # 3. CSS-—Å—Ç–∏–ª–∏ (—É—Å–∏–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    text = re.sub(r'[a-z\-]+\s*:\s*[^;"]+;?', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'\w+\s*=\s*["\'][^"\']*["\']', ' ', text)
    text = re.sub(r'\b\d+[a-z%]+\b', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'#[0-9a-f]{3,6}\b', ' ', text, flags=re.IGNORECASE)
    
    # 4. –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —á–∏—Å–ª–∞ –∏ CSS-—Å–ª–æ–≤–∞
    text = re.sub(r'\b(\d+)\s+\1\b', '', text) 
    text = re.sub(r'\b\d+px\b', '', text, flags=re.I)
    text = re.sub(r'\bcaps\b', '', text, flags=re.I)
    text = re.sub(r'\bstart\b', '', text, flags=re.I)  # "5px start"
    
    # 5. –ß–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def preprocess_text(text: str) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    if not isinstance(text, str) or not text.strip():
        return ""
    
    text = clean_html(text)
    text = text.lower()
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    words = []
    for w in text.split():
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        if (len(w) > 2 and 
            len(w) < 20 and  # ‚Üê –¥–æ–±–∞–≤–∏–ª–∏: –Ω–µ –±–æ–ª—å—à–µ 20 —Å–∏–º–≤–æ–ª–æ–≤
            w not in STOP_WORDS and
            not w.isdigit() and
            not re.match(r'^\d+$', w) and
            not re.match(r'^\d+[a-z]+$', w, re.I) and  # 3px, 255rgb
            not re.match(r'^[a-z]+\d+$', w, re.I) and  # comment_id, answer2
            not any(bad in w for bad in ['amp', 'comment', 'answer', 'mailto'])):  # –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
            
            # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤
            if re.match(r'^[–∞-—è—ë]+$', w):
                parsed = morph.parse(w)[0]
                w = parsed.normal_form
            words.append(w)
    
    return ' '.join(words)


def calculate_metrics(topics, cluster_names, topic_model):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫"""
    cluster_counts = Counter(topics)
    noise_count = cluster_counts.get(-1, 0)
    noise_percent = (noise_count / len(topics)) * 100 if len(topics) > 0 else 0
    n_clusters = len([c for c in cluster_counts.keys() if c != -1])
    cluster_sizes = [count for cluster, count in cluster_counts.items() if cluster != -1]
    avg_size = np.mean(cluster_sizes) if cluster_sizes else 0
    
    top_clusters = []
    sorted_clusters = sorted(
        [(cluster, count) for cluster, count in cluster_counts.items() if cluster != -1],
        key=lambda x: x[1],
        reverse=True
    )
    
    for cluster_id, size in sorted_clusters[:3]:
        name = cluster_names.get(cluster_id, f"Cluster {cluster_id}")
        top_clusters.append({
            'id': cluster_id,
            'name': name,
            'size': size
        })
    
    return {
        'n_clusters': n_clusters,
        'noise_percent': round(noise_percent, 2),
        'avg_cluster_size': round(avg_size, 2),
        'total_texts': len(topics),
        'top_clusters': top_clusters,
        'cluster_distribution': dict(cluster_counts)
    }


def clusterize_texts(file_path: str, progress_callback=None):
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    import time
    start_time = time.time()

    async def log_progress(msg):
        print(msg)
        if progress_callback:
            try:
                await progress_callback(msg)
            except:
                pass

    def sync_log(msg):
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(log_progress(msg))
            else:
                loop.run_until_complete(log_progress(msg))
        except:
            print(msg)

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ ---
    sync_log("üì• –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª...")
    df = pd.read_csv(file_path, usecols=[0], encoding='utf-8', dtype=str)
    raw_texts = df.iloc[:, 0].fillna("").astype(str).tolist()
    n = len(raw_texts)
    if n == 0:
        raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π")

    sync_log(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {n} —Ç–µ–∫—Å—Ç–æ–≤")

    # --- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ ---
    sync_log("üßπ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞...")
    preprocessed_texts = [preprocess_text(t) for t in raw_texts]
    
    valid_indices = [i for i, t in enumerate(preprocessed_texts) 
                     if t.strip() and len(t.split()) >= 2]
    
    if len(valid_indices) <= 3:
        df["cluster_id"] = 0
        df["cluster_name"] = "–í—Å–µ —Ç–µ–∫—Å—Ç—ã"
        out = file_path.replace(".csv", "_clustered.csv")
        df.to_csv(out, index=False, encoding='utf-8')
        return out, {'n_clusters': 1, 'total_texts': n}

    preprocessed_texts = [preprocessed_texts[i] for i in valid_indices]
    df = df.iloc[valid_indices].reset_index(drop=True)
    
    # --- –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ---
    sync_log("üîç –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    df = df.drop_duplicates(subset=df.columns[0], keep="first").reset_index(drop=True)
    unique_texts = df.iloc[:, 0].tolist()
    n_unique = len(unique_texts)
    sync_log(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {n_unique}")

    # --- –ú–æ–¥–µ–ª—å ---
    sync_log("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # +++ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤ +++
    vectorizer_model = CountVectorizer(
        ngram_range=(1, 2),
        stop_words=list(STOP_WORDS),
        min_df=3,  # —Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∏–Ω–∏–º—É–º –≤ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        max_df=0.6  # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ >60% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    )

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ~1000 —Ç–µ–∫—Å—Ç–æ–≤ ---
    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    if n_unique < 500:
        # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        min_cluster_size = 5
        min_samples = 2
        n_neighbors = 10
    elif n_unique < 5000:
        # –î–ª—è 500-5000 —Ç–µ–∫—Å—Ç–æ–≤ (—Ç–≤–æ–π —Å–ª—É—á–∞–π: 759)
        min_cluster_size = max(12, int(n_unique * 0.015))  # ~11 –¥–ª—è 759
        min_samples = max(3, int(min_cluster_size * 0.3))  # ~3-4
        n_neighbors = min(25, max(15, n_unique // 40))     # ~19
    else:
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (30–∫+)
        min_cluster_size = max(50, int(n_unique * 0.002))  # ~60 –¥–ª—è 30–∫
        min_samples = max(10, int(min_cluster_size * 0.2)) # ~12
        n_neighbors = min(50, max(30, n_unique // 200))    # ~150

    # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {n_unique} —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"   min_cluster_size = {min_cluster_size}")
    print(f"   min_samples = {min_samples}")
    print(f"   n_neighbors = {n_neighbors}")

    n_components = 10


    umap_model = UMAP(
        n_neighbors=n_neighbors,
        n_components=n_components,
        min_dist=0.0,
        metric='cosine',
        random_state=42,
        n_jobs=1
    )
    
    hdbscan_model = HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True
    )
    
    topic_model = BERTopic(
        embedding_model=model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer_model,  # +++ –î–û–ë–ê–í–õ–ï–ù–û +++
        language="multilingual",
        calculate_probabilities=False,
        verbose=False,
        top_n_words=10,
        n_gram_range=(1, 2),
        min_topic_size=min_cluster_size
    )

    # --- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ---
    sync_log(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (min_size={min_cluster_size})...")
    try:
        topics, _ = topic_model.fit_transform(unique_texts)
    except Exception as e:
        sync_log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        raise

    # --- –ù–∞–∑–≤–∞–Ω–∏—è (—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π) ---
    sync_log("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π...")
    info = topic_model.get_topic_info()
    cluster_names = {}
    
    def get_name(t):
        if t == -1:
            return "üîπ –ü—Ä–æ—á–µ–µ"
        
        topic_words = topic_model.get_topic(t)
        if not topic_words:
            cluster_names[t] = f"Cluster {t}"
            return f"Cluster {t}"
        
        filtered = []
        for word, score in topic_words:
            w_lower = word.lower()
            # –ñ–µ—Å—Ç–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if (w_lower not in STOP_WORDS and
                w_lower not in HTML_STOP_WORDS and
                len(word) > 2 and
                not word.isdigit() and
                not re.match(r'^\d+[a-z%]*$', word, re.I) and
                not re.match(r'^[a-z]{1,3}$', word)):  # –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–Ω–≥–ª —Å–ª–æ–≤–∞
                filtered.append(word)
            if len(filtered) >= 3:
                break
        
        if filtered:
            name = " ‚Ä¢ ".join(filtered[:3])
            cluster_names[t] = name
            return name
        
        cluster_names[t] = f"Cluster {t}"
        return f"Cluster {t}"

    df["cluster_id"] = topics
    df["cluster_name"] = [get_name(t) for t in topics]

    # --- –ú–µ—Ç—Ä–∏–∫–∏ ---
    stats = calculate_metrics(topics, cluster_names, topic_model)
    sync_log(f"‚úÖ {stats['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∑–∞ {time.time()-start_time:.1f}—Å")

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
    out = file_path.replace(".csv", "_clustered.csv")
    df.to_csv(out, index=False, encoding='utf-8')

    return out, stats
